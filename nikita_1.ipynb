{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nikita_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOdcAxXIFh/3aqGpIDgNJp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvanSedykh/nikita_task/blob/master/nikita_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxxQCk_1McIG",
        "colab_type": "code",
        "outputId": "81ae214d-8d2f-4af4-a798-e5f592dba053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        }
      },
      "source": [
        "!pip install gensim --upgrade"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b3/8358842ee8e430f7eb8f996bdd06c146a71712b9848ed32f949ad44b5adf/gensim-3.8.2-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 129kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.12.38)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.38)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.0.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.51.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.8.1->gensim) (2018.9)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjNHvZ7_WyY",
        "colab_type": "code",
        "outputId": "881e12d9-74f8-4089-e7c1-78586329ad45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!wget http://vectors.nlpl.eu/repository/20/187.zip\n",
        "!unzip 187.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-12 12:27:59--  http://vectors.nlpl.eu/repository/20/187.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2692389554 (2.5G) [application/zip]\n",
            "Saving to: ‘187.zip’\n",
            "\n",
            "187.zip             100%[===================>]   2.51G  17.1MB/s    in 2m 38s  \n",
            "\n",
            "2020-04-12 12:30:38 (16.3 MB/s) - ‘187.zip’ saved [2692389554/2692389554]\n",
            "\n",
            "Archive:  187.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.model             \n",
            "  inflating: model.model.vectors_ngrams.npy  \n",
            "  inflating: model.model.vectors.npy  \n",
            "  inflating: model.model.vectors_vocab.npy  \n",
            "  inflating: README                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvPMM43n_9th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "model_file = 'model.model'\n",
        "word2vec = gensim.models.KeyedVectors.load(model_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiS_2sdyYxAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDQIaBKWZuTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_excel('data_NN_2019.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-ExFFVuaWUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.fillna(0, inplace=True)\n",
        "data['Другое'] = data['Другое'].apply(lambda x:x if x==0 else 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-cgWHBGcRRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "def remove_punct(text):\n",
        "    text_nopunct = ''\n",
        "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
        "    return text_nopunct.lower()\n",
        "data['docs_Clean'] = data['docs'].apply(lambda x: remove_punct(x))\n",
        "data['tokens'] = data['docs_Clean'].apply(lambda x: x.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtpd0u2xoMcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "data_train, data_test = train_test_split(data, \n",
        "                                         test_size=0.2, \n",
        "                                         random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-0-2LOZm2ub",
        "colab_type": "code",
        "outputId": "e5efa8da-17af-467e-c841-1debf218ddf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51145 words total, with a vocabulary size of 10742\n",
            "Max sentence length is 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqx7y18Zn83s",
        "colab_type": "code",
        "outputId": "85df1ce4-c00c-4e1e-aaaf-43be11e033d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12471 words total, with a vocabulary size of 4077\n",
            "Max sentence length is 68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cEZiTz7o8xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4PkovB8p0vP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ULlqOOqM0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ts4WlC_rJIo",
        "colab_type": "code",
        "outputId": "60814335-75d6-4559-bcfe-48c7365a1e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import collections\n",
        "import re\n",
        "import string\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHOCrHJQqvGh",
        "colab_type": "code",
        "outputId": "ad2da35d-e180-4239-ea18-704214412573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train['docs_Clean'].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train['docs_Clean'].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10721 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytb2SuT2rDc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5Dkbgh_rt81",
        "colab_type": "code",
        "outputId": "8296eb82-9200-4443-c4b7-5b547623e6f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10722, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shUC4eZbryWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test['docs_Clean'].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6fSWu8or5KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYslYMC1sMN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_columns = ['id', 'docs','docs_Clean', 'tokens']\n",
        "y = data_train.drop(columns=feature_columns)\n",
        "label_names = y.columns\n",
        "y_train = y.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJeWpKrIsEAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzNBKwOltQPF",
        "colab_type": "code",
        "outputId": "c1abedbe-7b9a-4367-f4dd-7cef7f7db409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                len(list(label_names)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 50, 300)      3216600     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 49, 200)      120200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 48, 200)      180200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 47, 200)      240200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 46, 200)      300200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 45, 200)      360200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_6 (GlobalM (None, 200)          0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_7 (GlobalM (None, 200)          0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_8 (GlobalM (None, 200)          0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_9 (GlobalM (None, 200)          0           conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_10 (Global (None, 200)          0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1000)         0           global_max_pooling1d_6[0][0]     \n",
            "                                                                 global_max_pooling1d_7[0][0]     \n",
            "                                                                 global_max_pooling1d_8[0][0]     \n",
            "                                                                 global_max_pooling1d_9[0][0]     \n",
            "                                                                 global_max_pooling1d_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1000)         0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          128128      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 42)           5418        dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,551,146\n",
            "Trainable params: 1,334,546\n",
            "Non-trainable params: 3,216,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jh1RG8ptaV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 34"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Lqdkoxt236",
        "colab_type": "code",
        "outputId": "0d519fbb-457c-4e59-e8ba-4f048e1dca9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5366 samples, validate on 597 samples\n",
            "Epoch 1/3\n",
            "5366/5366 [==============================] - 2s 286us/step - loss: 0.2442 - acc: 0.9244 - val_loss: 0.1779 - val_acc: 0.9432\n",
            "Epoch 2/3\n",
            "5366/5366 [==============================] - 1s 230us/step - loss: 0.1670 - acc: 0.9445 - val_loss: 0.1602 - val_acc: 0.9481\n",
            "Epoch 3/3\n",
            "5366/5366 [==============================] - 1s 227us/step - loss: 0.1461 - acc: 0.9501 - val_loss: 0.1591 - val_acc: 0.9476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3hIkp3xuCFY",
        "colab_type": "code",
        "outputId": "2b8654a4-c74a-4576-b13b-e02f72f109f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1491/1491 [==============================] - 0s 45us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h_B1bGtuFta",
        "colab_type": "code",
        "outputId": "8f2d0383-447c-4efa-ab62-aa9af6ebee69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "array = predictions\n",
        "array[array >= 0.5] = 1\n",
        "array[array < 0.5] = 0\n",
        "array"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 1., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5z9QI7Au1o0",
        "colab_type": "code",
        "outputId": "3c1e772c-0ab6-4852-bcd3-792b1e89bdd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "data_test.values[0][2:-2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n",
              "       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
              "       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 1.0,\n",
              "       0.0, 0.0], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4feMdGuw0Hi",
        "colab_type": "code",
        "outputId": "b8dba4f7-2181-40ab-a10f-f2a2ab12997c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_correct = 0\n",
        "for y_true, y_pred in zip(data_test.values, predictions):\n",
        "    correct = (y_true[2:-2]==y_pred)\n",
        "    n_correct += correct.sum()\n",
        "accuracy = n_correct / (42 * 1491)\n",
        "print (accuracy )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9501453163424994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0o6YZG5z2Xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}